#!/bin/bash
# Serial sample script for Grid Engine
# Replace items enclosed by {}
#$ -S /bin/bash
#$ -q jaipur
#$ -N N$STEPNUMr$REPNUM
#$ -V
#$ -m aes
#$ -M notify@hyperblazer.net
#$ -cwd
#$ -notify

# Cluster-dependent setup (headers above are also cluster-dependent)
ONEWRAPPER_PY="/home/dwhs/OneWrapper/OneWrapper.py"

# most server-specific stuff goes in here
source "/home/dwhs/servers/carbon_serial.sh"

# ########################################################################
# ###### EVERYTHING BELOW HERE SHOULD BE THE SAME FOR ALL CLUSTERS #######
# ########################################################################

# Variables filled by gen_move.py script
STEP="$STEPNUM"
NEXT="$NEXTSTEP"
REP="$REPNUM"
CONF="$CONFFILE"
REPINFO="$REPFILE"
BASE="$MYBASE"

# TODO: everything below here can be made into a gromacs_shoot script, which
# can then be sourced in in order to separate gromacs-specific from
# cluster-specific

# run the program
# NOTE: $WORKDIR must be set in the startup or prologue
runprogram()
{
  # change to the scratch directory
  cd ${WORKDIR}

  # get filenames and generate directories for this and next tistraj
  TISTRAJSTR=`grep "^tistraj" ${BASE}/${CONF} | awk '{ print $2 }' | 
              sed 's/STEPNUM/STEP/'`
  TISTRAJ=`eval echo $TISTRAJSTR`
  mkdir -p `dirname $TISTRAJ`
  NEXT_TISTRAJ_STR=`echo $TISTRAJSTR | sed 's/\$STEP/\$NEXT/'`
  NEXT_TISTRAJ=`eval echo $NEXT_TISTRAJ_STR`
  mkdir -p `dirname $NEXT_TISTRAJ`

  echo "BASE = $BASE"
  echo "TISTRAJ = $TISTRAJ"
  echo "REPINFO = $REPINFO"

  # TODO: run the trajectory

  # TODO: make the tistraj from the trajectory
  touch $TISTRAJ 

  # TODO: check acceptance of the new trajectory based on the tistraj
  
  # copy (or symlink) most recently accepted trajectories to the right
  # location
  cp $TISTRAJ $NEXT_TISTRAJ
  
  # call OneWrapper again
  pushd $BASE
  ${ONEWRAPPER_PY} -N $NEXT -r $REPNUM $CONF 
  popd
}

startup     # start the script, output any info
prologue    # copy files to compute local nodes
runprogram  # do the real work
epilogue    # copy files back from the compute nodes
closing     # output on finishing script
exit
